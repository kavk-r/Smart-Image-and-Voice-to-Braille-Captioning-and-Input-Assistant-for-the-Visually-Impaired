{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35124a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='color:green'>[INFO] 09:19:23 - image_captioner - GPU detected: NVIDIA GeForce RTX 3060 Laptop GPU (Compute capability: 8.6)</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac89cec73f8c4ef5bf8775c96488188c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Advanced Image Caption Generator</h2>'), HBox(children=(VBox(children=(HTML(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import gc\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from IPython.display import display, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    AutoProcessor, \n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "from typing import Union, Tuple, List, Dict, Optional, Any\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Suppress deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Configure logger\n",
    "class NotebookLoggingHandler(logging.Handler):\n",
    "    \"\"\"Custom logging handler for displaying logs in Jupyter notebook with style\"\"\"\n",
    "    \n",
    "    def emit(self, record):\n",
    "        log_entry = self.format(record)\n",
    "        level = record.levelname\n",
    "        \n",
    "        if level == 'ERROR':\n",
    "            style = \"color:red; font-weight:bold\"\n",
    "        elif level == 'WARNING':\n",
    "            style = \"color:orange; font-weight:bold\"\n",
    "        elif level == 'INFO':\n",
    "            style = \"color:green\"\n",
    "        else:\n",
    "            style = \"color:gray\"\n",
    "            \n",
    "        output = f\"<div style='{style}'>[{level}] {log_entry}</div>\"\n",
    "        display(HTML(output))\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure advanced logging with custom handler\"\"\"\n",
    "    logger = logging.getLogger('image_captioner')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Add custom notebook handler for pretty display\n",
    "    notebook_handler = NotebookLoggingHandler()\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(message)s', datefmt='%H:%M:%S')\n",
    "    notebook_handler.setFormatter(formatter)\n",
    "    logger.addHandler(notebook_handler)\n",
    "    \n",
    "    # Add file handler for persistent logs\n",
    "    file_handler = logging.FileHandler('image_caption_generation.log')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    # Make sure we don't duplicate logs if logger already exists\n",
    "    logger.propagate = False\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logging()\n",
    "\n",
    "class GPUStatsMonitor:\n",
    "    \"\"\"Monitor GPU memory usage during processing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.has_gpu = torch.cuda.is_available()\n",
    "        if self.has_gpu:\n",
    "            self.device_count = torch.cuda.device_count()\n",
    "        else:\n",
    "            self.device_count = 0\n",
    "            \n",
    "    def log_memory_usage(self):\n",
    "        \"\"\"Log current GPU memory usage\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            logger.info(\"No GPU available\")\n",
    "            return\n",
    "            \n",
    "        for i in range(self.device_count):\n",
    "            total_mem = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB\n",
    "            reserved = torch.cuda.memory_reserved(i) / 1024**3  # GB\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB\n",
    "            free = total_mem - reserved\n",
    "            \n",
    "            logger.info(f\"GPU {i} - Total: {total_mem:.2f}GB | Used: {allocated:.2f}GB | \" \n",
    "                      f\"Reserved: {reserved:.2f}GB | Free: {free:.2f}GB\")\n",
    "            \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU cache\"\"\"\n",
    "        if self.has_gpu:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            logger.info(\"GPU memory cache cleared\")\n",
    "\n",
    "class ImageProcessor:\n",
    "    \"\"\"Advanced image processing utilities\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_image(\n",
    "        image_path: str, \n",
    "        resize_dim: Tuple[int, int] = (224, 224),\n",
    "        apply_normalization: bool = True,\n",
    "        enhance_contrast: bool = False,\n",
    "        contrast_factor: float = 1.2\n",
    "    ) -> Image.Image:\n",
    "        \"\"\"\n",
    "        Preprocess an image with advanced options\n",
    "        \n",
    "        Args:\n",
    "            image_path: Path to the image\n",
    "            resize_dim: Target resize dimensions\n",
    "            apply_normalization: Whether to normalize image\n",
    "            enhance_contrast: Whether to enhance contrast\n",
    "            contrast_factor: Contrast enhancement factor\n",
    "            \n",
    "        Returns:\n",
    "            Processed PIL Image\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Open and convert image to RGB\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Apply auto-contrast adjustment\n",
    "            if enhance_contrast:\n",
    "                image = ImageEnhance.Contrast(image).enhance(contrast_factor)\n",
    "                \n",
    "            # Resize image with antialiasing\n",
    "            image = image.resize(resize_dim, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            return image\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error preprocessing image: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_image(image: Image.Image, figsize: Tuple[int, int] = (8, 8)) -> None:\n",
    "        \"\"\"\n",
    "        Display the image with matplotlib\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image to display\n",
    "            figsize: Figure size for display\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    @staticmethod\n",
    "    def convert_to_tensor(\n",
    "        image: Image.Image, \n",
    "        device: torch.device,\n",
    "        normalize: bool = True\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Convert PIL image to normalized tensor\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            device: Target device\n",
    "            normalize: Whether to normalize using ImageNet stats\n",
    "            \n",
    "        Returns:\n",
    "            Image tensor on specified device\n",
    "        \"\"\"\n",
    "        if normalize:\n",
    "            # ImageNet normalization values\n",
    "            transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225]\n",
    "                )\n",
    "            ])\n",
    "            tensor = transform(image).unsqueeze(0).to(device)\n",
    "        else:\n",
    "            tensor = to_tensor(image).unsqueeze(0).to(device)\n",
    "            \n",
    "        return tensor\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Manage different image captioning models\"\"\"\n",
    "    \n",
    "    AVAILABLE_MODELS = {\n",
    "        \"blip-base\": {\n",
    "            \"name\": \"Salesforce/blip-image-captioning-base\",\n",
    "            \"type\": \"blip\",\n",
    "            \"desc\": \"Base BLIP model for image captioning\"\n",
    "        },\n",
    "        \"blip-large\": {\n",
    "            \"name\": \"Salesforce/blip-image-captioning-large\",\n",
    "            \"type\": \"blip\",\n",
    "            \"desc\": \"Large BLIP model for image captioning (better quality)\"\n",
    "        },\n",
    "        \"git-base\": {\n",
    "            \"name\": \"microsoft/git-base\",\n",
    "            \"type\": \"git\",\n",
    "            \"desc\": \"GenerativeImage2Text base model from Microsoft\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_key: str = \"blip-base\", device: Optional[Union[torch.device, str]] = None):\n",
    "        \"\"\"\n",
    "        Initialize model manager\n",
    "        \n",
    "        Args:\n",
    "            model_key: Key for the model to load\n",
    "            device: Target device, defaults to CUDA if available\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        elif isinstance(device, int):\n",
    "            self.device = torch.device(f\"cuda:{device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        else:\n",
    "            self.device = device if isinstance(device, torch.device) else torch.device(device)\n",
    "            \n",
    "        self.model_key = model_key\n",
    "        self.model_info = self.AVAILABLE_MODELS.get(model_key)\n",
    "        \n",
    "        if not self.model_info:\n",
    "            raise ValueError(f\"Unknown model: {model_key}. Available models: {list(self.AVAILABLE_MODELS.keys())}\")\n",
    "            \n",
    "        self.processor = None\n",
    "        self.model = None\n",
    "        \n",
    "    def load_model(self, use_half_precision: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Load the specified model and processor\n",
    "        \n",
    "        Args:\n",
    "            use_half_precision: Whether to use FP16 for faster inference with slight quality reduction\n",
    "        \"\"\"\n",
    "        model_name = self.model_info[\"name\"]\n",
    "        model_type = self.model_info[\"type\"]\n",
    "        \n",
    "        gpu_monitor = GPUStatsMonitor()\n",
    "        gpu_monitor.log_memory_usage()\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Loading {model_name} on {self.device}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            if model_type == \"blip\":\n",
    "                self.processor = BlipProcessor.from_pretrained(model_name)\n",
    "                self.model = BlipForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "            elif model_type == \"git\":\n",
    "                self.processor = AutoProcessor.from_pretrained(model_name)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(model_name).to(self.device)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "                \n",
    "            # Convert to half precision if requested and supported\n",
    "            if use_half_precision and self.device.type == \"cuda\" and torch.cuda.is_available():\n",
    "                if hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):\n",
    "                    self.model = self.model.half()\n",
    "                    logger.info(\"Model converted to half precision (FP16)\")\n",
    "                else:\n",
    "                    logger.warning(\"Half precision requested but not supported\")\n",
    "                    \n",
    "            elapsed = time.time() - start_time\n",
    "            logger.info(f\"Model loaded in {elapsed:.2f} seconds\")\n",
    "            gpu_monitor.log_memory_usage()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "            \n",
    "    def unload_model(self) -> None:\n",
    "        \"\"\"Unload model from memory\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model = None\n",
    "            self.processor = None\n",
    "            \n",
    "            gpu_monitor = GPUStatsMonitor()\n",
    "            gpu_monitor.clear_memory()\n",
    "            gpu_monitor.log_memory_usage()\n",
    "            \n",
    "            logger.info(\"Model unloaded from memory\")\n",
    "            \n",
    "    def generate_caption(\n",
    "        self, \n",
    "        image: Image.Image,\n",
    "        max_length: int = 30,\n",
    "        num_beams: int = 5,\n",
    "        min_length: int = 5,\n",
    "        top_p: float = 0.9,\n",
    "        repetition_penalty: float = 1.0,\n",
    "        temperature: float = 1.0,\n",
    "        num_return_sequences: int = 1\n",
    "    ) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Generate caption(s) for the given image\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image to caption\n",
    "            max_length: Maximum length of generated caption\n",
    "            num_beams: Number of beams for beam search\n",
    "            min_length: Minimum length of generated caption\n",
    "            top_p: Top-p sampling probability\n",
    "            repetition_penalty: Penalty for repeating tokens\n",
    "            temperature: Sampling temperature\n",
    "            num_return_sequences: Number of captions to generate\n",
    "            \n",
    "        Returns:\n",
    "            Caption string or list of captions\n",
    "        \"\"\"\n",
    "        if self.model is None or self.processor is None:\n",
    "            raise RuntimeError(\"Model and processor must be loaded first\")\n",
    "            \n",
    "        model_type = self.model_info[\"type\"]\n",
    "        \n",
    "        try:\n",
    "            # Process image\n",
    "            if model_type == \"blip\":\n",
    "                inputs = self.processor(images=image, return_tensors=\"pt\")\n",
    "                pixel_values = inputs.pixel_values.to(self.device)\n",
    "                \n",
    "                # Use torch.cuda.amp.autocast for mixed precision inference if available\n",
    "                if self.device.type == \"cuda\" and hasattr(torch.cuda, 'amp') and hasattr(torch.cuda.amp, 'autocast'):\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        with torch.no_grad():\n",
    "                            generated_ids = self.model.generate(\n",
    "                                pixel_values,\n",
    "                                max_length=max_length,\n",
    "                                min_length=min_length,\n",
    "                                num_beams=num_beams,\n",
    "                                top_p=top_p,\n",
    "                                repetition_penalty=repetition_penalty,\n",
    "                                temperature=temperature,\n",
    "                                num_return_sequences=num_return_sequences,\n",
    "                            )\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        generated_ids = self.model.generate(\n",
    "                            pixel_values,\n",
    "                            max_length=max_length,\n",
    "                            min_length=min_length,\n",
    "                            num_beams=num_beams,\n",
    "                            top_p=top_p,\n",
    "                            repetition_penalty=repetition_penalty,\n",
    "                            temperature=temperature,\n",
    "                            num_return_sequences=num_return_sequences,\n",
    "                        )\n",
    "                        \n",
    "                # Decode generated ids\n",
    "                if num_return_sequences > 1:\n",
    "                    captions = [self.processor.decode(output_ids, skip_special_tokens=True) \n",
    "                               for output_ids in generated_ids]\n",
    "                    return captions\n",
    "                else:\n",
    "                    caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "                    return caption\n",
    "                    \n",
    "            elif model_type == \"git\":\n",
    "                inputs = self.processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    generated_ids = self.model.generate(\n",
    "                        pixel_values=inputs.pixel_values,\n",
    "                        max_length=max_length,\n",
    "                        min_length=min_length,\n",
    "                        num_beams=num_beams,\n",
    "                        top_p=top_p,\n",
    "                        repetition_penalty=repetition_penalty,\n",
    "                        temperature=temperature,\n",
    "                        num_return_sequences=num_return_sequences,\n",
    "                    )\n",
    "                    \n",
    "                # Decode generated ids\n",
    "                if num_return_sequences > 1:\n",
    "                    captions = [self.processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "                               for output_ids in generated_ids]\n",
    "                    return captions\n",
    "                else:\n",
    "                    caption = self.processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "                    return caption\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating caption: {e}\")\n",
    "            raise\n",
    "            \n",
    "class ImageCaptioningApp:\n",
    "    \"\"\"Interactive image captioning application for Jupyter notebooks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the application\"\"\"\n",
    "        self.gpu_monitor = GPUStatsMonitor()\n",
    "        self.current_image_path = None\n",
    "        self.current_image = None\n",
    "        self.model_manager = None\n",
    "        self.model_key = \"blip-base\"  # Default model\n",
    "        self.use_half_precision = True\n",
    "        self.setup_ui()\n",
    "        \n",
    "    def setup_ui(self):\n",
    "        \"\"\"Set up interactive UI widgets\"\"\"\n",
    "        # Model selection\n",
    "        self.model_dropdown = widgets.Dropdown(\n",
    "            options=list(ModelManager.AVAILABLE_MODELS.keys()),\n",
    "            value=self.model_key,\n",
    "            description='Model:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Half precision toggle\n",
    "        self.half_precision_toggle = widgets.Checkbox(\n",
    "            value=self.use_half_precision,\n",
    "            description='Use half precision (FP16)',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        # Image upload widget\n",
    "        self.file_upload = widgets.FileUpload(\n",
    "            accept='.jpg,.jpeg,.png',\n",
    "            multiple=False,\n",
    "            description='Upload Image:',\n",
    "            layout=widgets.Layout(width='300px')\n",
    "        )\n",
    "        \n",
    "        # Caption parameters\n",
    "        self.max_length_slider = widgets.IntSlider(\n",
    "            value=30, min=10, max=100, step=5, \n",
    "            description='Max Length:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.num_beams_slider = widgets.IntSlider(\n",
    "            value=5, min=1, max=10, step=1, \n",
    "            description='Beam Search:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.temperature_slider = widgets.FloatSlider(\n",
    "            value=1.0, min=0.1, max=2.0, step=0.1, \n",
    "            description='Temperature:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        self.num_captions_slider = widgets.IntSlider(\n",
    "            value=1, min=1, max=5, step=1, \n",
    "            description='Caption Count:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Enhanced image processing options\n",
    "        self.enhance_contrast_toggle = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Enhance contrast',\n",
    "            disabled=False\n",
    "        )\n",
    "        \n",
    "        self.contrast_factor_slider = widgets.FloatSlider(\n",
    "            value=1.2, min=0.5, max=2.0, step=0.1, \n",
    "            description='Contrast:',\n",
    "            disabled=True,\n",
    "            style={'description_width': 'initial'},\n",
    "            layout=widgets.Layout(width='400px')\n",
    "        )\n",
    "        \n",
    "        # Link contrast toggle to slider\n",
    "        def toggle_contrast_slider(change):\n",
    "            self.contrast_factor_slider.disabled = not change['new']\n",
    "        self.enhance_contrast_toggle.observe(toggle_contrast_slider, names='value')\n",
    "        \n",
    "        # Button to load model\n",
    "        self.load_model_button = widgets.Button(\n",
    "            description='Load Model',\n",
    "            button_style='primary',\n",
    "            tooltip='Load the selected model'\n",
    "        )\n",
    "        self.load_model_button.on_click(self.handle_load_model)\n",
    "        \n",
    "        # Button to unload model\n",
    "        self.unload_model_button = widgets.Button(\n",
    "            description='Unload Model',\n",
    "            button_style='warning',\n",
    "            tooltip='Unload model from memory',\n",
    "            disabled=True\n",
    "        )\n",
    "        self.unload_model_button.on_click(self.handle_unload_model)\n",
    "        \n",
    "        # Button to generate caption\n",
    "        self.generate_button = widgets.Button(\n",
    "            description='Generate Caption',\n",
    "            button_style='success',\n",
    "            tooltip='Generate caption for the image',\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='200px')\n",
    "        )\n",
    "        self.generate_button.on_click(self.handle_generate_caption)\n",
    "        \n",
    "        # Output area\n",
    "        self.output_area = widgets.Output()\n",
    "        \n",
    "        # Handle file upload\n",
    "        self.file_upload.observe(self.handle_file_upload, names='value')\n",
    "        \n",
    "        # Layout all widgets\n",
    "        model_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Model Settings</h3>\"),\n",
    "            widgets.HBox([self.model_dropdown, self.half_precision_toggle]),\n",
    "            widgets.HBox([self.load_model_button, self.unload_model_button])\n",
    "        ])\n",
    "        \n",
    "        image_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Image Upload</h3>\"),\n",
    "            self.file_upload\n",
    "        ])\n",
    "        \n",
    "        caption_params = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Caption Parameters</h3>\"),\n",
    "            self.max_length_slider,\n",
    "            self.num_beams_slider,\n",
    "            self.temperature_slider,\n",
    "            self.num_captions_slider\n",
    "        ])\n",
    "        \n",
    "        image_processing = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Image Processing</h3>\"),\n",
    "            self.enhance_contrast_toggle,\n",
    "            self.contrast_factor_slider\n",
    "        ])\n",
    "        \n",
    "        generate_section = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Generate</h3>\"),\n",
    "            self.generate_button\n",
    "        ])\n",
    "        \n",
    "        # Main layout\n",
    "        self.main_ui = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>Advanced Image Caption Generator</h2>\"),\n",
    "            widgets.HBox([model_section, image_section]),\n",
    "            widgets.HBox([caption_params, image_processing, generate_section]),\n",
    "            widgets.HTML(\"<h3>Results</h3>\"),\n",
    "            self.output_area\n",
    "        ])\n",
    "        \n",
    "        # Display the UI\n",
    "        display(self.main_ui)\n",
    "        \n",
    "    def handle_load_model(self, button):\n",
    "        \"\"\"Handle model loading button click\"\"\"\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            self.model_key = self.model_dropdown.value\n",
    "            self.use_half_precision = self.half_precision_toggle.value\n",
    "            \n",
    "            try:\n",
    "                self.model_manager = ModelManager(self.model_key)\n",
    "                self.model_manager.load_model(use_half_precision=self.use_half_precision)\n",
    "                \n",
    "                self.load_model_button.disabled = True\n",
    "                self.unload_model_button.disabled = False\n",
    "                \n",
    "                # Enable generate button if we also have an image\n",
    "                if self.current_image is not None:\n",
    "                    self.generate_button.disabled = False\n",
    "                    \n",
    "                model_desc = ModelManager.AVAILABLE_MODELS[self.model_key][\"desc\"]\n",
    "                logger.info(f\"Model loaded successfully: {model_desc}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load model: {str(e)}\")\n",
    "                \n",
    "    def handle_unload_model(self, button):\n",
    "        \"\"\"Handle model unloading button click\"\"\"\n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            if self.model_manager:\n",
    "                self.model_manager.unload_model()\n",
    "                self.model_manager = None\n",
    "                \n",
    "            self.load_model_button.disabled = False\n",
    "            self.unload_model_button.disabled = True\n",
    "            self.generate_button.disabled = True\n",
    "            \n",
    "    def handle_file_upload(self, change):\n",
    "        \"\"\"Handle file upload changes\"\"\"\n",
    "        if not change.new:\n",
    "            return\n",
    "            \n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            try:\n",
    "                # Fix: Handle widget.FileUpload value properly\n",
    "                # In newer ipywidgets versions, the structure is different\n",
    "                if isinstance(change.new, tuple):\n",
    "                    # Handle case when change.new is a tuple (older version)\n",
    "                    if len(change.new) > 0 and isinstance(change.new[0], dict):\n",
    "                        uploaded_file = change.new[0]\n",
    "                    else:\n",
    "                        logger.error(\"Unexpected file upload format\")\n",
    "                        return\n",
    "                else:\n",
    "                    # Handle case when change.new is a dict with values() method\n",
    "                    uploaded_file = list(change.new.values())[0]\n",
    "                \n",
    "                # Extract content and filename\n",
    "                if 'content' in uploaded_file and 'name' in uploaded_file:\n",
    "                    content = uploaded_file['content']\n",
    "                    filename = uploaded_file['name']\n",
    "                else:\n",
    "                    logger.error(\"Uploaded file missing required metadata\")\n",
    "                    return\n",
    "                \n",
    "                # Save file temporarily\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(content)\n",
    "                    \n",
    "                self.current_image_path = filename\n",
    "                logger.info(f\"Image uploaded: {filename}\")\n",
    "                \n",
    "                # Load and display image\n",
    "                self.current_image = Image.open(self.current_image_path).convert(\"RGB\")\n",
    "                ImageProcessor.visualize_image(self.current_image)\n",
    "                \n",
    "                # Enable generate button if model is loaded\n",
    "                if self.model_manager is not None:\n",
    "                    self.generate_button.disabled = False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error handling file upload: {str(e)}\")\n",
    "                \n",
    "    def handle_generate_caption(self, button):\n",
    "        \"\"\"Handle generate caption button click\"\"\"\n",
    "        if not self.current_image_path or not self.model_manager:\n",
    "            return\n",
    "            \n",
    "        with self.output_area:\n",
    "            clear_output()\n",
    "            \n",
    "            try:\n",
    "                # Get parameter values\n",
    "                max_length = self.max_length_slider.value\n",
    "                num_beams = self.num_beams_slider.value\n",
    "                temperature = self.temperature_slider.value\n",
    "                num_captions = self.num_captions_slider.value\n",
    "                enhance_contrast = self.enhance_contrast_toggle.value\n",
    "                contrast_factor = self.contrast_factor_slider.value\n",
    "                \n",
    "                logger.info(\"Processing image...\")\n",
    "                \n",
    "                # Process image with selected options\n",
    "                processed_image = ImageProcessor.preprocess_image(\n",
    "                    self.current_image_path,\n",
    "                    resize_dim=(224, 224),\n",
    "                    enhance_contrast=enhance_contrast,\n",
    "                    contrast_factor=contrast_factor\n",
    "                )\n",
    "                \n",
    "                # Show processed image\n",
    "                logger.info(\"Processed image:\")\n",
    "                ImageProcessor.visualize_image(processed_image)\n",
    "                \n",
    "                # Generate caption\n",
    "                logger.info(\"Generating caption...\")\n",
    "                self.gpu_monitor.log_memory_usage()\n",
    "                \n",
    "                # Time the caption generation\n",
    "                start_time = time.time()\n",
    "                \n",
    "                captions = self.model_manager.generate_caption(\n",
    "                    processed_image,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    temperature=temperature,\n",
    "                    num_return_sequences=num_captions\n",
    "                )\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                logger.info(f\"Caption generation took {elapsed:.2f} seconds\")\n",
    "                \n",
    "                # Display results\n",
    "                if isinstance(captions, list):\n",
    "                    display(HTML(\"<h3>Generated Captions:</h3>\"))\n",
    "                    for i, caption in enumerate(captions):\n",
    "                        display(HTML(f\"<div style='margin-bottom:10px'><b>Caption {i+1}:</b> {caption}</div>\"))\n",
    "                else:\n",
    "                    display(HTML(f\"<h3>Generated Caption:</h3><div><b>{captions}</b></div>\"))\n",
    "                    \n",
    "                # Save caption to file\n",
    "                with open(f\"{os.path.splitext(self.current_image_path)[0]}_caption.txt\", \"w\") as f:\n",
    "                    if isinstance(captions, list):\n",
    "                        for i, caption in enumerate(captions):\n",
    "                            f.write(f\"Caption {i+1}: {caption}\\n\")\n",
    "                    else:\n",
    "                        f.write(captions)\n",
    "                \n",
    "                logger.info(\"Caption saved to file\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating caption: {str(e)}\")\n",
    "\n",
    "# Function to run the app in a Jupyter cell\n",
    "def run_image_caption_app():\n",
    "    \"\"\"Initialize and run the image captioning application\"\"\"\n",
    "    # Check for GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(device)\n",
    "        device_capability = torch.cuda.get_device_capability(device)\n",
    "        logger.info(f\"GPU detected: {device_name} (Compute capability: {device_capability[0]}.{device_capability[1]})\")\n",
    "    else:\n",
    "        logger.warning(\"No GPU detected. Using CPU. Processing will be slower.\")\n",
    "    \n",
    "    app = ImageCaptioningApp()\n",
    "    return app\n",
    "\n",
    "# For notebook direct execution\n",
    "if __name__ == \"__main__\":\n",
    "    run_image_caption_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26c41d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Image Quality Assessment & Auto‐Enhancement Pipeline</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1008fe145f4f5bbb7a12393ed2a80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.jpg,.jpeg,.png', description='Upload Image')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d075ff986d49de9a3f6d36fa3efe8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Auto-Enhancing Assessor with Iterative Exposure Fix & Download\n",
    "# ================================================================\n",
    "import cv2, numpy as np, os, tempfile, logging, matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "from IPython.display import display, HTML, clear_output, FileLink\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# ------------------------- Logging -------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "log = logging.getLogger(\"auto_assessor\")\n",
    "\n",
    "# ---------------------- Metric Framework ----------------------\n",
    "@dataclass\n",
    "class Threshold:\n",
    "    lower: float|None=None\n",
    "    upper: float|None=None\n",
    "    def check(self, v):\n",
    "        if self.lower is not None and v < self.lower: return False, f\"{v:.2f} < {self.lower}\"\n",
    "        if self.upper is not None and v > self.upper: return False, f\"{v:.2f} > {self.upper}\"\n",
    "        return True, \"\"\n",
    "\n",
    "class Metric:\n",
    "    name=\"base\"\n",
    "    def __init__(self, th:Threshold): self.th=th\n",
    "    def compute(self, gray, bgr): raise NotImplementedError\n",
    "    def __call__(self, gray, bgr):\n",
    "        val = self.compute(gray, bgr)\n",
    "        ok, reason = self.th.check(val)\n",
    "        return {\"metric\":self.name, \"value\":round(float(val),3), \"pass_\":ok, \"reason\":reason}\n",
    "\n",
    "class Brightness(Metric):  name=\"brightness\"; compute=lambda s,g,b: np.mean(g)\n",
    "class Contrast(Metric):    name=\"contrast\";   compute=lambda s,g,b: np.std(g)\n",
    "class SharpLap(Metric):    name=\"lap_var\";    compute=lambda s,g,b: cv2.Laplacian(g,cv2.CV_64F).var()\n",
    "class Entropy(Metric):\n",
    "    name=\"entropy\"\n",
    "    def compute(self, g, _):\n",
    "        hist=cv2.calcHist([g],[0],None,[256],[0,256]).flatten()\n",
    "        p=hist/hist.sum(); p=p[p>0]\n",
    "        return -(p*np.log2(p)).sum()\n",
    "class Exposure(Metric):\n",
    "    name=\"exposure\"\n",
    "    compute=lambda s,g,b: max(np.mean(g<30), np.mean(g>225))*100\n",
    "class Colorfulness(Metric):\n",
    "    name=\"color\"\n",
    "    def compute(self, _, bgr):\n",
    "        b,g,r = cv2.split(bgr.astype(float))\n",
    "        rg, yb = np.abs(r-g), np.abs(0.5*(r+g)-b)\n",
    "        return np.sqrt(rg.var()+yb.var()) + 0.3*np.sqrt(rg.mean()**2+yb.mean()**2)\n",
    "\n",
    "DEFAULT_THRESH = {\n",
    "    \"brightness\": Threshold(40,210),\n",
    "    \"contrast\":   Threshold(25,None),\n",
    "    \"lap_var\":    Threshold(120,None),\n",
    "    \"entropy\":    Threshold(4.5,None),\n",
    "    \"exposure\":   Threshold(None,12),\n",
    "    \"color\":      Threshold(10,None)\n",
    "}\n",
    "METRIC_CLS = dict(\n",
    "    brightness=Brightness, contrast=Contrast, lap_var=SharpLap,\n",
    "    entropy=Entropy, exposure=Exposure, color=Colorfulness\n",
    ")\n",
    "\n",
    "def _run_metrics(gray,bgr):\n",
    "    ms = [METRIC_CLS[n](t) for n,t in DEFAULT_THRESH.items()]\n",
    "    with ThreadPoolExecutor() as ex:\n",
    "        fut = [ex.submit(m,gray,bgr) for m in ms]\n",
    "        return [f.result() for f in as_completed(fut)]\n",
    "\n",
    "def assess_pil(img:Image.Image) -> dict:\n",
    "    arr = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    gray = cv2.cvtColor(arr, cv2.COLOR_BGR2GRAY)\n",
    "    res = _run_metrics(gray, arr)\n",
    "    res.sort(key=lambda x: x[\"metric\"])\n",
    "    return {\"overall_pass\": all(r[\"pass_\"] for r in res), \"metrics\": res}\n",
    "\n",
    "# --------------------- Enhancement Steps ---------------------\n",
    "def denoise_pil(img:Image.Image) -> Image.Image:\n",
    "    arr = np.array(img)\n",
    "    den = cv2.fastNlMeansDenoisingColored(arr, None, 10,10,7,21)\n",
    "    return Image.fromarray(den.astype(np.uint8))\n",
    "\n",
    "def sharpen_pil(img:Image.Image) -> Image.Image:\n",
    "    arr = np.array(img)\n",
    "    kernel = np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
    "    sharp = cv2.filter2D(arr, -1, kernel)\n",
    "    return Image.fromarray(np.clip(sharp,0,255).astype(np.uint8))\n",
    "\n",
    "def percentile_stretch(img:Image.Image, low:int, high:int) -> Image.Image:\n",
    "    arr = np.array(img)\n",
    "    yuv = cv2.cvtColor(arr, cv2.COLOR_RGB2YUV)\n",
    "    y = yuv[:,:,0]\n",
    "    lo, hi = np.percentile(y, low), np.percentile(y, high)\n",
    "    y2 = np.clip((y-lo)/(hi-lo)*255,0,255).astype(np.uint8)\n",
    "    yuv[:,:,0] = y2\n",
    "    rgb = cv2.cvtColor(yuv, cv2.COLOR_YUV2RGB)\n",
    "    return Image.fromarray(rgb)\n",
    "\n",
    "# ------------------- Display Helpers -------------------\n",
    "def show(img, size=(5,3)):\n",
    "    plt.figure(figsize=size); plt.imshow(img); plt.axis('off'); plt.show()\n",
    "\n",
    "def html_table(rep:dict) -> str:\n",
    "    rows = \"\".join(\n",
    "        f\"<tr><td>{r['metric']}</td>\"\n",
    "        f\"<td>{r['value']:.2f}</td>\"\n",
    "        f\"<td style='color:{'green' if r['pass_'] else 'red'}'>{'PASS' if r['pass_'] else 'FAIL'}</td>\"\n",
    "        f\"<td>{r['reason']}</td></tr>\"\n",
    "        for r in rep[\"metrics\"]\n",
    "    )\n",
    "    fg, txt = (\"green\",\"PASS ✅\") if rep[\"overall_pass\"] else (\"red\",\"FAIL ❌\")\n",
    "    return f\"\"\"\n",
    "    <table style='border-collapse:collapse;margin-top:10px'>\n",
    "      <tr><th>Metric</th><th>Value</th><th>Status</th><th>Reason</th></tr>\n",
    "      {rows}\n",
    "      <caption style='color:{fg};font-weight:bold'>{txt}</caption>\n",
    "    </table>\"\"\"\n",
    "\n",
    "# ---------------------- Upload Widget ----------------------\n",
    "upload = widgets.FileUpload(accept='.jpg,.jpeg,.png', multiple=False, description='Upload Image')\n",
    "out    = widgets.Output()\n",
    "\n",
    "def extract_meta(v):\n",
    "    if isinstance(v, dict): return next(iter(v.values()))\n",
    "    if isinstance(v, (list,tuple)): return v[0]\n",
    "    raise ValueError\n",
    "\n",
    "def handle_upload(change):\n",
    "    if not change.new: return\n",
    "    out.clear_output()\n",
    "    meta = extract_meta(upload.value)\n",
    "    data = meta['content']\n",
    "    fname = meta.get('name') or meta.get('metadata',{}).get('name','img')\n",
    "    src = os.path.join(tempfile.gettempdir(), fname)\n",
    "    with open(src,'wb') as f: f.write(data)\n",
    "    orig = Image.open(src).convert(\"RGB\")\n",
    "\n",
    "    with out:\n",
    "        # 1) Original\n",
    "        display(HTML(\"<h4>Original Image</h4>\"))\n",
    "        show(orig)\n",
    "        rep1 = assess_pil(orig); display(HTML(html_table(rep1)))\n",
    "\n",
    "        if not rep1[\"overall_pass\"]:\n",
    "            # 2) Denoise\n",
    "            dn = denoise_pil(orig)\n",
    "            display(HTML(\"<h4>Denoised Image</h4>\")); show(dn)\n",
    "            rep2 = assess_pil(dn); display(HTML(html_table(rep2)))\n",
    "\n",
    "            # 3) Sharpen\n",
    "            sp = sharpen_pil(dn)\n",
    "            display(HTML(\"<h4>Sharpened Image</h4>\")); show(sp)\n",
    "            rep3 = assess_pil(sp); display(HTML(html_table(rep3)))\n",
    "\n",
    "            # 4) Iterative exposure‐stretch\n",
    "            attempts = [(1,99),(2,98),(5,95),(10,90),(15,85),(20,80)]\n",
    "            final = None\n",
    "            for i,(low,high) in enumerate(attempts,1):\n",
    "                es = percentile_stretch(sp, low, high)\n",
    "                display(HTML(f\"<h4>Stretch Attempt {i}: low={low}% high={high}%</h4>\"))\n",
    "                show(es)\n",
    "                rep_es = assess_pil(es); display(HTML(html_table(rep_es)))\n",
    "                if rep_es[\"overall_pass\"]:\n",
    "                    final = es\n",
    "                    break\n",
    "            if final is None:\n",
    "                final = es  # last attempt\n",
    "\n",
    "            # 5) Save & Download\n",
    "            out_name = f\"enhanced_{fname}\"\n",
    "            out_path = os.path.join(os.getcwd(), out_name)\n",
    "            final.save(out_path)\n",
    "            display(FileLink(out_path, result_html_prefix=\"📥 Download final enhanced image: \"))\n",
    "\n",
    "upload.observe(handle_upload, names='value')\n",
    "display(HTML(\"<h3>Image Quality Assessment & Auto‐Enhancement Pipeline</h3>\"))\n",
    "display(upload, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hades",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
